==================================================
LLaMA2-7B 模型结构信息
==================================================

1. 所有参数名称和形状:
embed_tokens.weight: torch.Size([32000, 4096])
layers.0.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.0.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.0.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.0.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.0.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.0.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.0.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.0.input_layernorm.weight: torch.Size([4096])
layers.0.post_attention_layernorm.weight: torch.Size([4096])
layers.1.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.1.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.1.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.1.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.1.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.1.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.1.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.1.input_layernorm.weight: torch.Size([4096])
layers.1.post_attention_layernorm.weight: torch.Size([4096])
layers.2.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.2.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.2.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.2.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.2.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.2.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.2.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.2.input_layernorm.weight: torch.Size([4096])
layers.2.post_attention_layernorm.weight: torch.Size([4096])
layers.3.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.3.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.3.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.3.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.3.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.3.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.3.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.3.input_layernorm.weight: torch.Size([4096])
layers.3.post_attention_layernorm.weight: torch.Size([4096])
layers.4.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.4.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.4.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.4.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.4.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.4.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.4.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.4.input_layernorm.weight: torch.Size([4096])
layers.4.post_attention_layernorm.weight: torch.Size([4096])
layers.5.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.5.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.5.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.5.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.5.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.5.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.5.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.5.input_layernorm.weight: torch.Size([4096])
layers.5.post_attention_layernorm.weight: torch.Size([4096])
layers.6.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.6.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.6.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.6.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.6.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.6.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.6.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.6.input_layernorm.weight: torch.Size([4096])
layers.6.post_attention_layernorm.weight: torch.Size([4096])
layers.7.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.7.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.7.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.7.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.7.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.7.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.7.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.7.input_layernorm.weight: torch.Size([4096])
layers.7.post_attention_layernorm.weight: torch.Size([4096])
layers.8.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.8.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.8.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.8.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.8.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.8.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.8.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.8.input_layernorm.weight: torch.Size([4096])
layers.8.post_attention_layernorm.weight: torch.Size([4096])
layers.9.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.9.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.9.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.9.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.9.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.9.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.9.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.9.input_layernorm.weight: torch.Size([4096])
layers.9.post_attention_layernorm.weight: torch.Size([4096])
layers.10.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.10.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.10.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.10.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.10.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.10.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.10.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.10.input_layernorm.weight: torch.Size([4096])
layers.10.post_attention_layernorm.weight: torch.Size([4096])
layers.11.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.11.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.11.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.11.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.11.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.11.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.11.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.11.input_layernorm.weight: torch.Size([4096])
layers.11.post_attention_layernorm.weight: torch.Size([4096])
layers.12.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.12.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.12.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.12.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.12.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.12.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.12.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.12.input_layernorm.weight: torch.Size([4096])
layers.12.post_attention_layernorm.weight: torch.Size([4096])
layers.13.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.13.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.13.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.13.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.13.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.13.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.13.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.13.input_layernorm.weight: torch.Size([4096])
layers.13.post_attention_layernorm.weight: torch.Size([4096])
layers.14.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.14.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.14.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.14.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.14.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.14.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.14.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.14.input_layernorm.weight: torch.Size([4096])
layers.14.post_attention_layernorm.weight: torch.Size([4096])
layers.15.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.15.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.15.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.15.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.15.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.15.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.15.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.15.input_layernorm.weight: torch.Size([4096])
layers.15.post_attention_layernorm.weight: torch.Size([4096])
layers.16.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.16.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.16.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.16.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.16.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.16.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.16.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.16.input_layernorm.weight: torch.Size([4096])
layers.16.post_attention_layernorm.weight: torch.Size([4096])
layers.17.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.17.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.17.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.17.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.17.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.17.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.17.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.17.input_layernorm.weight: torch.Size([4096])
layers.17.post_attention_layernorm.weight: torch.Size([4096])
layers.18.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.18.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.18.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.18.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.18.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.18.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.18.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.18.input_layernorm.weight: torch.Size([4096])
layers.18.post_attention_layernorm.weight: torch.Size([4096])
layers.19.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.19.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.19.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.19.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.19.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.19.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.19.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.19.input_layernorm.weight: torch.Size([4096])
layers.19.post_attention_layernorm.weight: torch.Size([4096])
layers.20.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.20.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.20.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.20.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.20.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.20.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.20.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.20.input_layernorm.weight: torch.Size([4096])
layers.20.post_attention_layernorm.weight: torch.Size([4096])
layers.21.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.21.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.21.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.21.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.21.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.21.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.21.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.21.input_layernorm.weight: torch.Size([4096])
layers.21.post_attention_layernorm.weight: torch.Size([4096])
layers.22.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.22.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.22.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.22.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.22.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.22.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.22.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.22.input_layernorm.weight: torch.Size([4096])
layers.22.post_attention_layernorm.weight: torch.Size([4096])
layers.23.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.23.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.23.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.23.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.23.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.23.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.23.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.23.input_layernorm.weight: torch.Size([4096])
layers.23.post_attention_layernorm.weight: torch.Size([4096])
layers.24.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.24.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.24.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.24.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.24.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.24.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.24.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.24.input_layernorm.weight: torch.Size([4096])
layers.24.post_attention_layernorm.weight: torch.Size([4096])
layers.25.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.25.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.25.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.25.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.25.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.25.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.25.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.25.input_layernorm.weight: torch.Size([4096])
layers.25.post_attention_layernorm.weight: torch.Size([4096])
layers.26.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.26.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.26.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.26.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.26.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.26.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.26.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.26.input_layernorm.weight: torch.Size([4096])
layers.26.post_attention_layernorm.weight: torch.Size([4096])
layers.27.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.27.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.27.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.27.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.27.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.27.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.27.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.27.input_layernorm.weight: torch.Size([4096])
layers.27.post_attention_layernorm.weight: torch.Size([4096])
layers.28.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.28.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.28.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.28.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.28.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.28.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.28.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.28.input_layernorm.weight: torch.Size([4096])
layers.28.post_attention_layernorm.weight: torch.Size([4096])
layers.29.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.29.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.29.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.29.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.29.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.29.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.29.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.29.input_layernorm.weight: torch.Size([4096])
layers.29.post_attention_layernorm.weight: torch.Size([4096])
layers.30.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.30.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.30.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.30.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.30.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.30.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.30.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.30.input_layernorm.weight: torch.Size([4096])
layers.30.post_attention_layernorm.weight: torch.Size([4096])
layers.31.self_attn.q_proj.weight: torch.Size([4096, 4096])
layers.31.self_attn.k_proj.weight: torch.Size([4096, 4096])
layers.31.self_attn.v_proj.weight: torch.Size([4096, 4096])
layers.31.self_attn.o_proj.weight: torch.Size([4096, 4096])
layers.31.mlp.gate_proj.weight: torch.Size([11008, 4096])
layers.31.mlp.up_proj.weight: torch.Size([11008, 4096])
layers.31.mlp.down_proj.weight: torch.Size([4096, 11008])
layers.31.input_layernorm.weight: torch.Size([4096])
layers.31.post_attention_layernorm.weight: torch.Size([4096])
norm.weight: torch.Size([4096])

2. 所有模块:
: LlamaModel
embed_tokens: Embedding
layers: ModuleList
layers.0: LlamaDecoderLayer
layers.0.self_attn: LlamaAttention
layers.0.self_attn.q_proj: Linear
layers.0.self_attn.k_proj: Linear
layers.0.self_attn.v_proj: Linear
layers.0.self_attn.o_proj: Linear
layers.0.mlp: LlamaMLP
layers.0.mlp.gate_proj: Linear
layers.0.mlp.up_proj: Linear
layers.0.mlp.down_proj: Linear
layers.0.mlp.act_fn: SiLUActivation
layers.0.input_layernorm: LlamaRMSNorm
layers.0.post_attention_layernorm: LlamaRMSNorm
layers.1: LlamaDecoderLayer
layers.1.self_attn: LlamaAttention
layers.1.self_attn.q_proj: Linear
layers.1.self_attn.k_proj: Linear
layers.1.self_attn.v_proj: Linear
layers.1.self_attn.o_proj: Linear
layers.1.mlp: LlamaMLP
layers.1.mlp.gate_proj: Linear
layers.1.mlp.up_proj: Linear
layers.1.mlp.down_proj: Linear
layers.1.mlp.act_fn: SiLUActivation
layers.1.input_layernorm: LlamaRMSNorm
layers.1.post_attention_layernorm: LlamaRMSNorm
layers.2: LlamaDecoderLayer
layers.2.self_attn: LlamaAttention
layers.2.self_attn.q_proj: Linear
layers.2.self_attn.k_proj: Linear
layers.2.self_attn.v_proj: Linear
layers.2.self_attn.o_proj: Linear
layers.2.mlp: LlamaMLP
layers.2.mlp.gate_proj: Linear
layers.2.mlp.up_proj: Linear
layers.2.mlp.down_proj: Linear
layers.2.mlp.act_fn: SiLUActivation
layers.2.input_layernorm: LlamaRMSNorm
layers.2.post_attention_layernorm: LlamaRMSNorm
layers.3: LlamaDecoderLayer
layers.3.self_attn: LlamaAttention
layers.3.self_attn.q_proj: Linear
layers.3.self_attn.k_proj: Linear
layers.3.self_attn.v_proj: Linear
layers.3.self_attn.o_proj: Linear
layers.3.mlp: LlamaMLP
layers.3.mlp.gate_proj: Linear
layers.3.mlp.up_proj: Linear
layers.3.mlp.down_proj: Linear
layers.3.mlp.act_fn: SiLUActivation
layers.3.input_layernorm: LlamaRMSNorm
layers.3.post_attention_layernorm: LlamaRMSNorm
layers.4: LlamaDecoderLayer
layers.4.self_attn: LlamaAttention
layers.4.self_attn.q_proj: Linear
layers.4.self_attn.k_proj: Linear
layers.4.self_attn.v_proj: Linear
layers.4.self_attn.o_proj: Linear
layers.4.mlp: LlamaMLP
layers.4.mlp.gate_proj: Linear
layers.4.mlp.up_proj: Linear
layers.4.mlp.down_proj: Linear
layers.4.mlp.act_fn: SiLUActivation
layers.4.input_layernorm: LlamaRMSNorm
layers.4.post_attention_layernorm: LlamaRMSNorm
layers.5: LlamaDecoderLayer
layers.5.self_attn: LlamaAttention
layers.5.self_attn.q_proj: Linear
layers.5.self_attn.k_proj: Linear
layers.5.self_attn.v_proj: Linear
layers.5.self_attn.o_proj: Linear
layers.5.mlp: LlamaMLP
layers.5.mlp.gate_proj: Linear
layers.5.mlp.up_proj: Linear
layers.5.mlp.down_proj: Linear
layers.5.mlp.act_fn: SiLUActivation
layers.5.input_layernorm: LlamaRMSNorm
layers.5.post_attention_layernorm: LlamaRMSNorm
layers.6: LlamaDecoderLayer
layers.6.self_attn: LlamaAttention
layers.6.self_attn.q_proj: Linear
layers.6.self_attn.k_proj: Linear
layers.6.self_attn.v_proj: Linear
layers.6.self_attn.o_proj: Linear
layers.6.mlp: LlamaMLP
layers.6.mlp.gate_proj: Linear
layers.6.mlp.up_proj: Linear
layers.6.mlp.down_proj: Linear
layers.6.mlp.act_fn: SiLUActivation
layers.6.input_layernorm: LlamaRMSNorm
layers.6.post_attention_layernorm: LlamaRMSNorm
layers.7: LlamaDecoderLayer
layers.7.self_attn: LlamaAttention
layers.7.self_attn.q_proj: Linear
layers.7.self_attn.k_proj: Linear
layers.7.self_attn.v_proj: Linear
layers.7.self_attn.o_proj: Linear
layers.7.mlp: LlamaMLP
layers.7.mlp.gate_proj: Linear
layers.7.mlp.up_proj: Linear
layers.7.mlp.down_proj: Linear
layers.7.mlp.act_fn: SiLUActivation
layers.7.input_layernorm: LlamaRMSNorm
layers.7.post_attention_layernorm: LlamaRMSNorm
layers.8: LlamaDecoderLayer
layers.8.self_attn: LlamaAttention
layers.8.self_attn.q_proj: Linear
layers.8.self_attn.k_proj: Linear
layers.8.self_attn.v_proj: Linear
layers.8.self_attn.o_proj: Linear
layers.8.mlp: LlamaMLP
layers.8.mlp.gate_proj: Linear
layers.8.mlp.up_proj: Linear
layers.8.mlp.down_proj: Linear
layers.8.mlp.act_fn: SiLUActivation
layers.8.input_layernorm: LlamaRMSNorm
layers.8.post_attention_layernorm: LlamaRMSNorm
layers.9: LlamaDecoderLayer
layers.9.self_attn: LlamaAttention
layers.9.self_attn.q_proj: Linear
layers.9.self_attn.k_proj: Linear
layers.9.self_attn.v_proj: Linear
layers.9.self_attn.o_proj: Linear
layers.9.mlp: LlamaMLP
layers.9.mlp.gate_proj: Linear
layers.9.mlp.up_proj: Linear
layers.9.mlp.down_proj: Linear
layers.9.mlp.act_fn: SiLUActivation
layers.9.input_layernorm: LlamaRMSNorm
layers.9.post_attention_layernorm: LlamaRMSNorm
layers.10: LlamaDecoderLayer
layers.10.self_attn: LlamaAttention
layers.10.self_attn.q_proj: Linear
layers.10.self_attn.k_proj: Linear
layers.10.self_attn.v_proj: Linear
layers.10.self_attn.o_proj: Linear
layers.10.mlp: LlamaMLP
layers.10.mlp.gate_proj: Linear
layers.10.mlp.up_proj: Linear
layers.10.mlp.down_proj: Linear
layers.10.mlp.act_fn: SiLUActivation
layers.10.input_layernorm: LlamaRMSNorm
layers.10.post_attention_layernorm: LlamaRMSNorm
layers.11: LlamaDecoderLayer
layers.11.self_attn: LlamaAttention
layers.11.self_attn.q_proj: Linear
layers.11.self_attn.k_proj: Linear
layers.11.self_attn.v_proj: Linear
layers.11.self_attn.o_proj: Linear
layers.11.mlp: LlamaMLP
layers.11.mlp.gate_proj: Linear
layers.11.mlp.up_proj: Linear
layers.11.mlp.down_proj: Linear
layers.11.mlp.act_fn: SiLUActivation
layers.11.input_layernorm: LlamaRMSNorm
layers.11.post_attention_layernorm: LlamaRMSNorm
layers.12: LlamaDecoderLayer
layers.12.self_attn: LlamaAttention
layers.12.self_attn.q_proj: Linear
layers.12.self_attn.k_proj: Linear
layers.12.self_attn.v_proj: Linear
layers.12.self_attn.o_proj: Linear
layers.12.mlp: LlamaMLP
layers.12.mlp.gate_proj: Linear
layers.12.mlp.up_proj: Linear
layers.12.mlp.down_proj: Linear
layers.12.mlp.act_fn: SiLUActivation
layers.12.input_layernorm: LlamaRMSNorm
layers.12.post_attention_layernorm: LlamaRMSNorm
layers.13: LlamaDecoderLayer
layers.13.self_attn: LlamaAttention
layers.13.self_attn.q_proj: Linear
layers.13.self_attn.k_proj: Linear
layers.13.self_attn.v_proj: Linear
layers.13.self_attn.o_proj: Linear
layers.13.mlp: LlamaMLP
layers.13.mlp.gate_proj: Linear
layers.13.mlp.up_proj: Linear
layers.13.mlp.down_proj: Linear
layers.13.mlp.act_fn: SiLUActivation
layers.13.input_layernorm: LlamaRMSNorm
layers.13.post_attention_layernorm: LlamaRMSNorm
layers.14: LlamaDecoderLayer
layers.14.self_attn: LlamaAttention
layers.14.self_attn.q_proj: Linear
layers.14.self_attn.k_proj: Linear
layers.14.self_attn.v_proj: Linear
layers.14.self_attn.o_proj: Linear
layers.14.mlp: LlamaMLP
layers.14.mlp.gate_proj: Linear
layers.14.mlp.up_proj: Linear
layers.14.mlp.down_proj: Linear
layers.14.mlp.act_fn: SiLUActivation
layers.14.input_layernorm: LlamaRMSNorm
layers.14.post_attention_layernorm: LlamaRMSNorm
layers.15: LlamaDecoderLayer
layers.15.self_attn: LlamaAttention
layers.15.self_attn.q_proj: Linear
layers.15.self_attn.k_proj: Linear
layers.15.self_attn.v_proj: Linear
layers.15.self_attn.o_proj: Linear
layers.15.mlp: LlamaMLP
layers.15.mlp.gate_proj: Linear
layers.15.mlp.up_proj: Linear
layers.15.mlp.down_proj: Linear
layers.15.mlp.act_fn: SiLUActivation
layers.15.input_layernorm: LlamaRMSNorm
layers.15.post_attention_layernorm: LlamaRMSNorm
layers.16: LlamaDecoderLayer
layers.16.self_attn: LlamaAttention
layers.16.self_attn.q_proj: Linear
layers.16.self_attn.k_proj: Linear
layers.16.self_attn.v_proj: Linear
layers.16.self_attn.o_proj: Linear
layers.16.mlp: LlamaMLP
layers.16.mlp.gate_proj: Linear
layers.16.mlp.up_proj: Linear
layers.16.mlp.down_proj: Linear
layers.16.mlp.act_fn: SiLUActivation
layers.16.input_layernorm: LlamaRMSNorm
layers.16.post_attention_layernorm: LlamaRMSNorm
layers.17: LlamaDecoderLayer
layers.17.self_attn: LlamaAttention
layers.17.self_attn.q_proj: Linear
layers.17.self_attn.k_proj: Linear
layers.17.self_attn.v_proj: Linear
layers.17.self_attn.o_proj: Linear
layers.17.mlp: LlamaMLP
layers.17.mlp.gate_proj: Linear
layers.17.mlp.up_proj: Linear
layers.17.mlp.down_proj: Linear
layers.17.mlp.act_fn: SiLUActivation
layers.17.input_layernorm: LlamaRMSNorm
layers.17.post_attention_layernorm: LlamaRMSNorm
layers.18: LlamaDecoderLayer
layers.18.self_attn: LlamaAttention
layers.18.self_attn.q_proj: Linear
layers.18.self_attn.k_proj: Linear
layers.18.self_attn.v_proj: Linear
layers.18.self_attn.o_proj: Linear
layers.18.mlp: LlamaMLP
layers.18.mlp.gate_proj: Linear
layers.18.mlp.up_proj: Linear
layers.18.mlp.down_proj: Linear
layers.18.mlp.act_fn: SiLUActivation
layers.18.input_layernorm: LlamaRMSNorm
layers.18.post_attention_layernorm: LlamaRMSNorm
layers.19: LlamaDecoderLayer
layers.19.self_attn: LlamaAttention
layers.19.self_attn.q_proj: Linear
layers.19.self_attn.k_proj: Linear
layers.19.self_attn.v_proj: Linear
layers.19.self_attn.o_proj: Linear
layers.19.mlp: LlamaMLP
layers.19.mlp.gate_proj: Linear
layers.19.mlp.up_proj: Linear
layers.19.mlp.down_proj: Linear
layers.19.mlp.act_fn: SiLUActivation
layers.19.input_layernorm: LlamaRMSNorm
layers.19.post_attention_layernorm: LlamaRMSNorm
layers.20: LlamaDecoderLayer
layers.20.self_attn: LlamaAttention
layers.20.self_attn.q_proj: Linear
layers.20.self_attn.k_proj: Linear
layers.20.self_attn.v_proj: Linear
layers.20.self_attn.o_proj: Linear
layers.20.mlp: LlamaMLP
layers.20.mlp.gate_proj: Linear
layers.20.mlp.up_proj: Linear
layers.20.mlp.down_proj: Linear
layers.20.mlp.act_fn: SiLUActivation
layers.20.input_layernorm: LlamaRMSNorm
layers.20.post_attention_layernorm: LlamaRMSNorm
layers.21: LlamaDecoderLayer
layers.21.self_attn: LlamaAttention
layers.21.self_attn.q_proj: Linear
layers.21.self_attn.k_proj: Linear
layers.21.self_attn.v_proj: Linear
layers.21.self_attn.o_proj: Linear
layers.21.mlp: LlamaMLP
layers.21.mlp.gate_proj: Linear
layers.21.mlp.up_proj: Linear
layers.21.mlp.down_proj: Linear
layers.21.mlp.act_fn: SiLUActivation
layers.21.input_layernorm: LlamaRMSNorm
layers.21.post_attention_layernorm: LlamaRMSNorm
layers.22: LlamaDecoderLayer
layers.22.self_attn: LlamaAttention
layers.22.self_attn.q_proj: Linear
layers.22.self_attn.k_proj: Linear
layers.22.self_attn.v_proj: Linear
layers.22.self_attn.o_proj: Linear
layers.22.mlp: LlamaMLP
layers.22.mlp.gate_proj: Linear
layers.22.mlp.up_proj: Linear
layers.22.mlp.down_proj: Linear
layers.22.mlp.act_fn: SiLUActivation
layers.22.input_layernorm: LlamaRMSNorm
layers.22.post_attention_layernorm: LlamaRMSNorm
layers.23: LlamaDecoderLayer
layers.23.self_attn: LlamaAttention
layers.23.self_attn.q_proj: Linear
layers.23.self_attn.k_proj: Linear
layers.23.self_attn.v_proj: Linear
layers.23.self_attn.o_proj: Linear
layers.23.mlp: LlamaMLP
layers.23.mlp.gate_proj: Linear
layers.23.mlp.up_proj: Linear
layers.23.mlp.down_proj: Linear
layers.23.mlp.act_fn: SiLUActivation
layers.23.input_layernorm: LlamaRMSNorm
layers.23.post_attention_layernorm: LlamaRMSNorm
layers.24: LlamaDecoderLayer
layers.24.self_attn: LlamaAttention
layers.24.self_attn.q_proj: Linear
layers.24.self_attn.k_proj: Linear
layers.24.self_attn.v_proj: Linear
layers.24.self_attn.o_proj: Linear
layers.24.mlp: LlamaMLP
layers.24.mlp.gate_proj: Linear
layers.24.mlp.up_proj: Linear
layers.24.mlp.down_proj: Linear
layers.24.mlp.act_fn: SiLUActivation
layers.24.input_layernorm: LlamaRMSNorm
layers.24.post_attention_layernorm: LlamaRMSNorm
layers.25: LlamaDecoderLayer
layers.25.self_attn: LlamaAttention
layers.25.self_attn.q_proj: Linear
layers.25.self_attn.k_proj: Linear
layers.25.self_attn.v_proj: Linear
layers.25.self_attn.o_proj: Linear
layers.25.mlp: LlamaMLP
layers.25.mlp.gate_proj: Linear
layers.25.mlp.up_proj: Linear
layers.25.mlp.down_proj: Linear
layers.25.mlp.act_fn: SiLUActivation
layers.25.input_layernorm: LlamaRMSNorm
layers.25.post_attention_layernorm: LlamaRMSNorm
layers.26: LlamaDecoderLayer
layers.26.self_attn: LlamaAttention
layers.26.self_attn.q_proj: Linear
layers.26.self_attn.k_proj: Linear
layers.26.self_attn.v_proj: Linear
layers.26.self_attn.o_proj: Linear
layers.26.mlp: LlamaMLP
layers.26.mlp.gate_proj: Linear
layers.26.mlp.up_proj: Linear
layers.26.mlp.down_proj: Linear
layers.26.mlp.act_fn: SiLUActivation
layers.26.input_layernorm: LlamaRMSNorm
layers.26.post_attention_layernorm: LlamaRMSNorm
layers.27: LlamaDecoderLayer
layers.27.self_attn: LlamaAttention
layers.27.self_attn.q_proj: Linear
layers.27.self_attn.k_proj: Linear
layers.27.self_attn.v_proj: Linear
layers.27.self_attn.o_proj: Linear
layers.27.mlp: LlamaMLP
layers.27.mlp.gate_proj: Linear
layers.27.mlp.up_proj: Linear
layers.27.mlp.down_proj: Linear
layers.27.mlp.act_fn: SiLUActivation
layers.27.input_layernorm: LlamaRMSNorm
layers.27.post_attention_layernorm: LlamaRMSNorm
layers.28: LlamaDecoderLayer
layers.28.self_attn: LlamaAttention
layers.28.self_attn.q_proj: Linear
layers.28.self_attn.k_proj: Linear
layers.28.self_attn.v_proj: Linear
layers.28.self_attn.o_proj: Linear
layers.28.mlp: LlamaMLP
layers.28.mlp.gate_proj: Linear
layers.28.mlp.up_proj: Linear
layers.28.mlp.down_proj: Linear
layers.28.mlp.act_fn: SiLUActivation
layers.28.input_layernorm: LlamaRMSNorm
layers.28.post_attention_layernorm: LlamaRMSNorm
layers.29: LlamaDecoderLayer
layers.29.self_attn: LlamaAttention
layers.29.self_attn.q_proj: Linear
layers.29.self_attn.k_proj: Linear
layers.29.self_attn.v_proj: Linear
layers.29.self_attn.o_proj: Linear
layers.29.mlp: LlamaMLP
layers.29.mlp.gate_proj: Linear
layers.29.mlp.up_proj: Linear
layers.29.mlp.down_proj: Linear
layers.29.mlp.act_fn: SiLUActivation
layers.29.input_layernorm: LlamaRMSNorm
layers.29.post_attention_layernorm: LlamaRMSNorm
layers.30: LlamaDecoderLayer
layers.30.self_attn: LlamaAttention
layers.30.self_attn.q_proj: Linear
layers.30.self_attn.k_proj: Linear
layers.30.self_attn.v_proj: Linear
layers.30.self_attn.o_proj: Linear
layers.30.mlp: LlamaMLP
layers.30.mlp.gate_proj: Linear
layers.30.mlp.up_proj: Linear
layers.30.mlp.down_proj: Linear
layers.30.mlp.act_fn: SiLUActivation
layers.30.input_layernorm: LlamaRMSNorm
layers.30.post_attention_layernorm: LlamaRMSNorm
layers.31: LlamaDecoderLayer
layers.31.self_attn: LlamaAttention
layers.31.self_attn.q_proj: Linear
layers.31.self_attn.k_proj: Linear
layers.31.self_attn.v_proj: Linear
layers.31.self_attn.o_proj: Linear
layers.31.mlp: LlamaMLP
layers.31.mlp.gate_proj: Linear
layers.31.mlp.up_proj: Linear
layers.31.mlp.down_proj: Linear
layers.31.mlp.act_fn: SiLUActivation
layers.31.input_layernorm: LlamaRMSNorm
layers.31.post_attention_layernorm: LlamaRMSNorm
norm: LlamaRMSNorm
rotary_emb: LlamaRotaryEmbedding
============================================================
1. 模型类型和基本信息
============================================================
模型类型: <class 'transformers.models.llama.modeling_llama.LlamaModel'>
模型类名: LlamaModel
配置类名: LlamaConfig

============================================================
2. LLaMA2 Model 的所有属性
============================================================
公共属性和方法:
  - T_destination
  - active_adapters
  - add_adapter
  - add_memory_hooks
  - add_model_tags
  - add_module
  - apply
  - base_model
  - base_model_prefix
  - bfloat16
  - buffers
  - call_super_init
  - can_generate
  - can_record_outputs
  - children
  - compile
  - config
  - config_class
  - cpu
  - create_extended_attention_mask_for_decoder

============================================================
3. 具体属性值探索
============================================================
model.model: 不存在
model.layers: <class 'torch.nn.modules.container.ModuleList'>
model.embed_tokens: <class 'torch.nn.modules.sparse.Embedding'>
model.norm: <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>

3. Transformer层详细结构:
模型总层数: 32
隐藏层维度: 4096
注意力头数: 32
KV头数: 32
每个头的维度: 128

第一层包含:
  - self_attn: LlamaAttention
  - mlp: LlamaMLP
  - input_layernorm: LlamaRMSNorm
  - post_attention_layernorm: LlamaRMSNorm

第一层注意力块包含:
  - q_proj: Linear
  - k_proj: Linear
  - v_proj: Linear
  - o_proj: Linear

第一层前馈网络包含:
  - gate_proj: Linear
  - up_proj: Linear
  - down_proj: Linear
  - act_fn: SiLUActivation

LLaMADecoderLayer forward 参数:
  hidden_states: hidden_states: torch.Tensor
  attention_mask: attention_mask: Optional[torch.Tensor] = None
  position_ids: position_ids: Optional[torch.LongTensor] = None
  past_key_values: past_key_values: Optional[transformers.cache_utils.Cache] = None
  use_cache: use_cache: Optional[bool] = False
  cache_position: cache_position: Optional[torch.LongTensor] = None
  position_embeddings: position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None